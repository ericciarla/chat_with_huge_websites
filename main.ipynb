{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericciarla/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import caching\n",
    "from dotenv import load_dotenv\n",
    "from firecrawl import FirecrawlApp\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "\n",
    "# Configure the Google Generative AI module with the API key\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "# Initialize the FirecrawlApp with your API key\n",
    "app = FirecrawlApp(api_key=firecrawl_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m crawl_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://dify.ai/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m    \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrawlOptions\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      7\u001b[0m     }\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m crawl_result \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawl_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrawl_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m crawl_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Convert crawl results to JSON format, excluding 'content' field from each entry\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     cleaned_crawl_result \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m crawl_result]\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/firecrawl/firecrawl.py:156\u001b[0m, in \u001b[0;36mFirecrawlApp.crawl_url\u001b[0;34m(self, url, params, wait_until_done, poll_interval, idempotency_key)\u001b[0m\n\u001b[1;32m    154\u001b[0m job_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobId\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait_until_done:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_monitor_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobId\u001b[39m\u001b[38;5;124m'\u001b[39m: job_id}\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/firecrawl/firecrawl.py:286\u001b[0m, in \u001b[0;36mFirecrawlApp._monitor_job_status\u001b[0;34m(self, job_id, headers, poll_interval)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m status_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaused\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpending\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqueued\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwaiting\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    285\u001b[0m     poll_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(poll_interval,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for the specified interval before checking again\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrawl job failed or was stopped. Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Crawl a website\n",
    "crawl_url = 'https://dify.ai/'\n",
    "params = {\n",
    "   \n",
    "    'crawlOptions': {\n",
    "        'limit': 100\n",
    "    }\n",
    "}\n",
    "crawl_result = app.crawl_url(crawl_url, params=params)\n",
    "\n",
    "if crawl_result is not None:\n",
    "    # Convert crawl results to JSON format, excluding 'content' field from each entry\n",
    "    cleaned_crawl_result = [{k: v for k, v in entry.items() if k != 'content'} for entry in crawl_result]\n",
    "\n",
    "    # Save the modified results as a text file containing JSON data\n",
    "    with open('crawl_result.txt', 'w') as file:\n",
    "        file.write(json.dumps(cleaned_crawl_result, indent=4))\n",
    "else:\n",
    "    print(\"No data returned from crawl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the video using the Files API\n",
    "text_file = genai.upload_file(path=\"crawl_result.txt\")\n",
    "\n",
    "# Wait for the file to finish processing\n",
    "while text_file.state.name == \"PROCESSING\":\n",
    "    print('Waiting for file to be processed.')\n",
    "    time.sleep(2)\n",
    "    text_file = genai.get_file(text_file.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cache with a 5 minute TTL\n",
    "cache = caching.CachedContent.create(\n",
    "    model=\"models/gemini-1.5-pro-001\",\n",
    "    display_name=\"website crawl testing again\", # used to identify the cache\n",
    "    system_instruction=\"You are an expert at this website, and your job is to answer user's query based on the website you have access to.\",\n",
    "    contents=[text_file],\n",
    "    ttl=datetime.timedelta(minutes=15),\n",
    ")\n",
    "# Construct a GenerativeModel which uses the created cache.\n",
    "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to query a website with Dify depends on your specific needs and the complexity of the information you want to retrieve. Dify offers several methods for accessing web data, each with its own advantages and drawbacks:\n",
      "\n",
      "**1. Web Crawler (Web Scraper) Tool:**\n",
      "\n",
      "* **Ideal for:** Extracting structured data, such as product information, pricing, and reviews, from a single website or a limited number of websites.\n",
      "* **How it works:** You provide Dify with a website URL and specify the data you want to extract using a simple configuration interface. Dify will then automatically crawl the website and return the desired data in a structured format.\n",
      "* **Pros:** Easy to use, no coding required, good for specific data extraction.\n",
      "* **Cons:** Not suitable for large-scale web crawling or complex data structures, may not work well with dynamic websites that require JavaScript rendering.\n",
      "\n",
      "**2. Google Search Tool:**\n",
      "\n",
      "* **Ideal for:** Getting general information from the web, such as news updates, summaries of topics, or definitions.\n",
      "* **How it works:** You can use the Google search tool within Dify's AI Agent or Chat capabilities to perform searches and retrieve relevant information from the web. The results can then be used as context for further reasoning or actions within your application.\n",
      "* **Pros:** Easy to use, good for general information retrieval, integrates seamlessly with Dify's AI Agent and Chat features.\n",
      "* **Cons:** Not suitable for extracting structured data or specific information from a single website, limited to Google search results, may not be as comprehensive as a web crawler.\n",
      "\n",
      "**3. External Data API Tools:**\n",
      "\n",
      "* **Ideal for:** Retrieving specific data from external APIs in real-time, such as weather information, Google search results, or Amazon product details.\n",
      "* **How it works:** You can configure Dify to access external APIs using the External Data API Tools. This allows you to directly integrate live data from these sources into your applications.\n",
      "* **Pros:** Provides real-time data access, good for specific data retrieval, integrates seamlessly with Dify's applications.\n",
      "* **Cons:** Requires configuring API keys and understanding API documentation, not suitable for general web browsing or unstructured data extraction.\n",
      "\n",
      "**4. Code-Based Extensions (for self-hosted Dify):**\n",
      "\n",
      "* **Ideal for:** Highly customized web data retrieval, potentially including dynamic websites or data manipulation that requires specific code.\n",
      "* **How it works:** Dify's self-hosted version allows developers to create custom code-based extensions to extend its functionality. You can write Python code to interact with websites, scrape data, or perform specific data processing.\n",
      "* **Pros:** Offers maximum flexibility and customization, allows for complex data manipulation, suitable for specific web data retrieval scenarios.\n",
      "* **Cons:** Requires programming knowledge, not suitable for non-technical users, may require more time and effort to develop and maintain.\n",
      "\n",
      "**Recommendation:**\n",
      "\n",
      "* **For basic web data retrieval, start with the Web Crawler tool.** It's easy to use and doesn't require coding.\n",
      "* **For general information retrieval, use the Google search tool.** It integrates seamlessly with Dify's AI Agent and Chat features.\n",
      "* **For specific data retrieval from external APIs, utilize the External Data API Tools.** This provides real-time data access and flexibility.\n",
      "* **If you need to retrieve complex data from websites or require highly customized web data manipulation, consider using Code-Based Extensions in the self-hosted version of Dify.** \n",
      "\n",
      "Remember that the best approach depends on your specific use case and the nature of the website you're interacting with.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the model\n",
    "response = model.generate_content([\"What is the best way to query a website with Dify?\"])\n",
    "response_dict = response.to_dict()\n",
    "response_text = response_dict['candidates'][0]['content']['parts'][0]['text']\n",
    "print(response_text)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
